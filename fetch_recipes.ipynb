{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, InvalidSelectorException, ElementNotVisibleException, WebDriverException\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_url = 'https://www.allrecipes.com/recipes/80/main-dish/?page='\n",
    "\n",
    "def search_entire_category(category_url, save_filename):\n",
    "    '''writes all recipe links for a given category to pickle list file'''\n",
    "    MAX_NUM_PAGES = 3 #TODO: find a way to set that automatically\n",
    "    recipe_urls = []\n",
    "    \n",
    "    for page in range(1, MAX_NUM_PAGES):\n",
    "        recipe_urls.extend(get_recipe_urls(category_url+str(page)))\n",
    "    \n",
    "    with open(save_filename, 'wb') as handle:\n",
    "        pickle.dump(recipe_urls, handle, protocol=pickle.HIGHEST_PROTOCOL)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe_urls(cat_page_url):\n",
    "    '''returns all recipe links for a given search page within a category'''\n",
    "    response = requests.get(cat_page_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    recipe_urls = []\n",
    "    \n",
    "    for elt in set(soup.find_all('a', href=re.compile('^(https://www.allrecipes\\.com\\/recipe\\/)[0-9]*(\\/)'))):\n",
    "        recipe_urls.append(elt['href'])\n",
    "                   \n",
    "    return list(set(recipe_urls))\n",
    "\n",
    "# cat_page_url = 'https://www.allrecipes.com/recipes/80/main-dish/?page=1'\n",
    "# print(get_recipe_urls(cat_page_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe_with_reviews_soup(recipe_url):\n",
    "    '''returns recipe features and recipe reviews'''\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('window-size=800x841')\n",
    "    options.add_argument('headless')\n",
    "    driver = webdriver.Chrome(chrome_options=options)\n",
    "    driver.get(weblink)\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    N_REVIEW_PAGES = 10\n",
    "    \n",
    "    for _ in range(N_REVIEW_PAGES):\n",
    "        try:\n",
    "            clicker = driver.find_element_by_css_selector('#reviews > div.recipe-reviews__more-container > div.more-button')\n",
    "            clicker.location_once_scrolled_into_view\n",
    "            clicker.click()\n",
    "            time.sleep(1)\n",
    "        except (TimeoutException, NoSuchElementException, InvalidSelectorException, ElementNotVisibleException, WebDriverException):\n",
    "            break\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.quit()\n",
    "    return soup\n",
    "\n",
    "def get_recipe_reviews(recipe_soup):\n",
    "    authors = [\" \".join(author.text.split()) for author in soup.find_all('h4', attrs={'itemprop': 'author'})]\n",
    "    ratings = [content['content'] for content in soup.find_all('meta', attrs={'itemprop': 'ratingValue'})][1:] #the first one is the average rating\n",
    "    dates = ([str(date['content']) for date in soup.find_all('meta', attrs={'itemprop': 'dateCreated'})])\n",
    "    reviews = list(set(zip(authors, ratings, dates)))\n",
    "    return reviews\n",
    "    \n",
    "def get_recipe_features(recipe_soup):\n",
    "    title = soup.find('h1', attrs={'id': 'recipe-main-content'}).text\n",
    "    breadcrumbs = [\" \".join(breadcrumb.text.split()) for breadcrumb in soup.find_all('span', attrs={'itemprop': 'name'})]\n",
    "#   TODO: do we want categories? there's also cuisines but both might be empty depending on the recipe\n",
    "#     categories = [\" \".join(category.text.split()) for category in soup.find_all('span', attrs={'itemprop': 'recipeCategory'})]\n",
    "    ingredients = [\" \".join(ingredient.text.split()) for ingredient in soup.find_all('span', attrs={'itemprop': 'recipeIngredient'})]\n",
    "    return title, breadcrumbs, ingredients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_db_from_recipe_urls(recipe_urls):\n",
    "    \n",
    "    for recipe_url in recipe_urls:\n",
    "        soup = get_recipe_with_reviews_soup(recipe_url)\n",
    "        \n",
    "        recipe_id = re.search('^(https://www.allrecipes\\.com\\/recipe\\/)([0-9]*)(\\/)', recipe_url, re.IGNORECASE).group(2)\n",
    "        reviews = get_recipe_reviews(soup)  \n",
    "        title, breadcrumbs, ingredients = get_recipe_features(soup)\n",
    "        \n",
    "        #TODO: PUT IN DATABASE SOMEHOW\n",
    "        \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
